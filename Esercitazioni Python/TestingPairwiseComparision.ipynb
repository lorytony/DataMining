{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a419770e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(150, 4) (150,)\n",
      "{'fit_time': array([0.00406837, 0.01268077, 0.00132465, 0.00553656]), 'score_time': array([0.00244713, 0.00239849, 0.00931883, 0.00243211]), 'test_precision_macro': array([1.        , 0.97619048, 0.97435897, 0.97619048]), 'test_recall_macro': array([1.        , 0.97222222, 0.97435897, 0.97222222]), 'test_accuracy': array([1.        , 0.97368421, 0.97297297, 0.97297297])}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import recall_score,accuracy_score\n",
    "\n",
    "X,y = datasets.load_iris(return_X_y=True)\n",
    "print(type(X))\n",
    "print(X.shape,y.shape)\n",
    "scoring = ['precision_macro','recall_macro','accuracy']\n",
    "classifier = svm.SVC(kernel='linear',C=1,random_state=10)\n",
    "scores = cross_validate(classifier,X,y,scoring=scoring,cv=4)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "11dc635d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RepeatedKFold' object has no attribute 'scoring'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-07382f86b00c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#non funge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrkf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'RepeatedKFold' object has no attribute 'scoring'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RepeatedKFold\n",
    "random_state = 12883823\n",
    "\n",
    "rkf = RepeatedKFold(n_splits=3, n_repeats=2, random_state=random_state)\n",
    "\n",
    "#non funge\n",
    "for result in rkf.scoring:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "faf01ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.02631579 0.02702703 0.02702703]\n",
      "[0.         0.02631579 0.02702703 0.02702703]\n",
      "[0.         0.02631579 0.02702703 0.02702703]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "risultati = []\n",
    "\n",
    "for i in range(1,4):\n",
    "    risultati.append(cross_validate(classifier,X,y,scoring=scoring,cv=4))\n",
    "\n",
    "for i in range(0,3):\n",
    "    print(1 - risultati[i]['test_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "82fded7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 2) (5,)\n",
      "[[1 6]\n",
      " [1 3]\n",
      " [1 4]\n",
      " [2 4]\n",
      " [3 1]] [1 0 1 0 0]\n",
      "[[3 1]\n",
      " [1 3]\n",
      " [2 4]\n",
      " [1 6]\n",
      " [1 4]] [0 0 0 1 1]\n",
      "[[3 1]\n",
      " [1 6]\n",
      " [2 4]\n",
      " [1 3]\n",
      " [1 4]] [0 1 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "#in effetti lo shuffle si limita a fare una corretta permutazione\n",
    "#senza perdere corrispondenze tra X e Y\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "ar1 = np.array([[1, 3],[1, 4],[2, 4],[3, 1],[1,6]])\n",
    "ar2 = np.array([0,1,0,0,1])\n",
    "\n",
    "print(ar1.shape,ar2.shape)\n",
    "\n",
    "for i in range(1,4):\n",
    "    ar1,ar2 = shuffle(ar1,ar2,random_state=1234*i)\n",
    "    print(ar1,ar2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f9a30ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.97368421 1.         0.94594595 1.        ]\n",
      "[0.97368421 0.97368421 0.97297297 1.        ]\n",
      "[1.         0.94736842 1.         0.94594595]\n",
      "[0.97368421 0.97368421 1.         0.94594595]\n",
      "[0.97368421 0.94736842 0.94594595 0.97297297]\n",
      "[0.97368421 0.97368421 0.97297297 0.97297297]\n",
      "---------------------------------\n",
      "[0.9799075391180655, 0.9733285917496444, 0.9599928876244666]\n",
      "[0.9800853485064012, 0.9733285917496445, 0.9733285917496444]\n"
     ]
    }
   ],
   "source": [
    "#calcoliamo il t-student\n",
    "#2)fare 5 modelli (quelli di Ale ;)\n",
    "X,y\n",
    "results_A = []\n",
    "results_B = []\n",
    "model_A ={\"Accuracy\":[]}\n",
    "model_B ={\"Accuracy\":[]}\n",
    "for i in range(1,4):\n",
    "    Xa,ya = shuffle(X,y,random_state=1234*i)\n",
    "    Xb,yb = shuffle(X,y,random_state=4321*i)\n",
    "    \n",
    "    results_A.append(cross_validate(classifier,Xa,ya,scoring=scoring,cv=4))\n",
    "    results_B.append(cross_validate(classifier,Xb,yb,scoring=scoring,cv=4))\n",
    "\n",
    "for i in range(0,3):\n",
    "    print(results_A[i]['test_accuracy'])\n",
    "    print(results_B[i]['test_accuracy'])\n",
    "print(\"---------------------------------\")    \n",
    "for result in results_A:\n",
    "    model_A[\"Accuracy\"].append(np.mean(result['test_accuracy']))\n",
    "for result in results_B:\n",
    "    model_B[\"Accuracy\"].append(np.mean(result['test_accuracy']))\n",
    "    \n",
    "print(model_A[\"Accuracy\"])\n",
    "print(model_B[\"Accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05b435e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(150, 4) (150,)\n",
      "[0.02       0.02       0.02       0.04       0.02666667 0.02\n",
      " 0.02       0.01333333 0.02666667 0.02      ]\n",
      "[0.02666667 0.02666667 0.02       0.02       0.02       0.01333333\n",
      " 0.02       0.02       0.02       0.02      ]\n",
      "0.022666666666666602\n",
      "0.02066666666666659\n",
      "0.7989354619369626\n",
      "t-distribution value is:2.262\n",
      "The Null Hypothesis can not be rejected. The two classifiers are statistically different by the chance.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import recall_score,accuracy_score\n",
    "\n",
    "X,y = datasets.load_iris(return_X_y=True)\n",
    "print(type(X))\n",
    "print(X.shape,y.shape)\n",
    "scoring = ['precision_macro','recall_macro','accuracy']\n",
    "classifier = svm.SVC(kernel='linear',C=1,random_state=10)\n",
    "\n",
    "\n",
    "import math\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "n_rounds = 10 #degree of freedom 9 (= 10-1)\n",
    "err_A = []\n",
    "err_B = []\n",
    "overall_err_A = 0.0\n",
    "overall_err_B = 0.0\n",
    "\n",
    "results_A = []\n",
    "results_B = []\n",
    "model_A ={\"Accuracy\":[]}\n",
    "model_B ={\"Accuracy\":[]}\n",
    "\n",
    "for rounds in range(1,n_rounds+1):\n",
    "    Xa,ya = shuffle(X,y,random_state=1234*rounds)\n",
    "    Xb,yb = shuffle(X,y,random_state=4321*rounds)#da eliminare nella versione finale\n",
    "    \n",
    "    results_A.append(cross_validate(classifier,Xa,ya,scoring=scoring,cv=10))\n",
    "    #results_B.append(cross_validate(classifier,Xa,ya,scoring=scoring,cv=10)) #codice vero\n",
    "    results_B.append(cross_validate(classifier,Xb,yb,scoring=scoring,cv=10)) #codice for testing\n",
    "\n",
    "for result in results_A:\n",
    "    model_A[\"Accuracy\"].append(np.mean(result['test_accuracy']))\n",
    "for result in results_B:\n",
    "    model_B[\"Accuracy\"].append(np.mean(result['test_accuracy']))\n",
    "\n",
    "err_A = 1 - np.array(model_A[\"Accuracy\"])\n",
    "err_B = 1 - np.array(model_B[\"Accuracy\"])\n",
    "\n",
    "print(err_A)\n",
    "print(err_B)\n",
    "\n",
    "overall_err_A = np.mean(err_A)\n",
    "overall_err_B = np.mean(err_B)\n",
    "\n",
    "print(overall_err_A)\n",
    "print(overall_err_B)\n",
    "\n",
    "\n",
    "partial_varAB = 0.0\n",
    "for i in range(0,n_rounds):\n",
    "    partial_varAB = partial_varAB + pow((err_A[i]-err_B[i]-(overall_err_A-overall_err_B)),2)\n",
    "\n",
    "varAB=1/n_rounds * partial_varAB\n",
    "\n",
    "t_test = (overall_err_A-overall_err_B)/math.sqrt(varAB/n_rounds)\n",
    "\n",
    "print(t_test)\n",
    "\n",
    "#t-table\n",
    "t_table = pd.read_csv(\"t-table.csv\")\n",
    "t_distribution = t_table.loc[9,\"0.025\"]\n",
    "\n",
    "print(\"t-distribution value is:{}\".format(t_distribution))\n",
    "if(t_test < -t_distribution or t_test > t_distribution):\n",
    "    print(\"The Null Hypothesis can be rejected. The two classifiers are statistically significant different.\")\n",
    "else:\n",
    "    print(\"The Null Hypothesis can not be rejected. The two classifiers are statistically different by the chance.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0040c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#FUNZIONE DEFINITIVA. \n",
    "#Ricorda di importare le opportune librerie!!\n",
    "\n",
    "def pairwiseComparisionModel(pipeline_A,pipeline_B,dataset,labels,n_rounds):\n",
    "    if(n_rounds <= 1):\n",
    "        print(\"Errore:number of rounds at least 2\")\n",
    "        return \n",
    "    \n",
    "        \n",
    "    degFredoom = n_rounds - 1 #degrees of freedom\n",
    "    err_A = []\n",
    "    err_B = []\n",
    "    overall_err_A = 0.0\n",
    "    overall_err_B = 0.0\n",
    "\n",
    "    results_A = []\n",
    "    results_B = []\n",
    "    model_A ={\"Accuracy\":[]}\n",
    "    model_B ={\"Accuracy\":[]}\n",
    "    \n",
    "    for rounds in range(1,n_rounds+1):\n",
    "        Xa,ya = shuffle(X,y,random_state=123456789*rounds)\n",
    "        results_A.append(cross_validate(estimator=pipeline_A,X=Xa,y=ya,scoring=scoring,cv=10))\n",
    "        results_B.append(cross_validate(estimator=pipeline_B,X=Xa,y=ya,scoring=scoring,cv=10))\n",
    "    \n",
    "    for result in results_A:\n",
    "        model_A[\"Accuracy\"].append(np.mean(result['test_accuracy']))\n",
    "    for result in results_B:\n",
    "        model_B[\"Accuracy\"].append(np.mean(result['test_accuracy']))\n",
    "\n",
    "    err_A = 1 - np.array(model_A[\"Accuracy\"])\n",
    "    err_B = 1 - np.array(model_B[\"Accuracy\"])\n",
    "\n",
    "    print(err_A)\n",
    "    print(err_B)\n",
    "\n",
    "    overall_err_A = np.mean(err_A)\n",
    "    overall_err_B = np.mean(err_B)\n",
    "\n",
    "    print(overall_err_A)\n",
    "    print(overall_err_B)\n",
    "\n",
    "\n",
    "    partial_varAB = 0.0\n",
    "    for i in range(0,n_rounds):\n",
    "        partial_varAB = partial_varAB + pow((err_A[i]-err_B[i]-(overall_err_A-overall_err_B)),2)\n",
    "\n",
    "    varAB=1/n_rounds * partial_varAB\n",
    "\n",
    "    t_test = (overall_err_A-overall_err_B)/math.sqrt(varAB/n_rounds)\n",
    "\n",
    "    print(t_test)\n",
    "\n",
    "    #t-table\n",
    "    t_table = pd.read_csv(\"t-table.csv\")\n",
    "    t_distribution = t_table.loc[degFreedom,\"0.025\"]\n",
    "\n",
    "    print(\"t-distribution value is:{}\".format(t_distribution))\n",
    "    if(t_test < -t_distribution or t_test > t_distribution):\n",
    "        print(\"The Null Hypothesis can be rejected. The two classifiers are statistically significant different.\")\n",
    "    else:\n",
    "        print(\"The Null Hypothesis can not be rejected. The two classifiers are statistically different by the chance.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cafdef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd36f31b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
